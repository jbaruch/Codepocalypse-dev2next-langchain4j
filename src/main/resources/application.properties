# Application Configuration
quarkus.application.name=airline-loyalty-assistant

# Quarkus LangChain4j Configuration for Docker Model Runner (Local Model)
# Using Docker Desktop Model Runner with Mistral 7B model
# Docker Model Runner exposes an OpenAI-compatible API on port 12434
# Reference: https://docs.docker.com/ai/model-runner/api-reference/
quarkus.langchain4j.openai.base-url=http://localhost:12434/engines/llama.cpp/v1
quarkus.langchain4j.openai.api-key=not-needed
quarkus.langchain4j.openai.chat-model.model-name=ai/mistral:7B-Q4_K_M
quarkus.langchain4j.openai.chat-model.temperature=0.7
quarkus.langchain4j.openai.timeout=60s
quarkus.langchain4j.openai.log-requests=true
quarkus.langchain4j.openai.log-responses=true

# Previous OpenAI Cloud Configuration (commented out for Stage 05)
# quarkus.langchain4j.openai.api-key=${OPENAI_API_KEY}
# quarkus.langchain4j.openai.chat-model.model-name=gpt-4o-mini
# quarkus.langchain4j.openai.chat-model.temperature=0.7
# quarkus.langchain4j.openai.timeout=30s

# Conversation Memory Configuration
# Use message window memory to retain recent conversation history
quarkus.langchain4j.chat-memory.type=message-window
# Keep last 20 messages in memory (10 exchanges)
quarkus.langchain4j.chat-memory.memory-window.max-messages=20

# MCP Configuration
# Configure MCP client to connect to local MCP server
quarkus.langchain4j.mcp.airline-tools.transport-type=http
quarkus.langchain4j.mcp.airline-tools.url=http://localhost:8080/mcp/sse
quarkus.langchain4j.mcp.airline-tools.log-requests=true
quarkus.langchain4j.mcp.airline-tools.log-responses=true

# Logging Configuration
quarkus.log.level=INFO
quarkus.log.category."dev2next.langchain4j".level=INFO
quarkus.log.category."io.quarkiverse.langchain4j".level=DEBUG

# HTTP Configuration
quarkus.http.port=8080
